---
title: "logistic_biplot_script"
author: "Camilo Estrella"
date: "2024-02-20"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries

We will start by loading the libraries that will be used, whose function is explained below:

```{r libraries, warning = FALSE, message = FALSE}
library(MultBiplotR)
library(knitr)
library(dplyr)
library(ggplot2)
library(scales)
```

* **MultBiplotR**: It allows multivariate analysis using logistic regression and its graphical representation employing a Biplot.
* **knitr**: The *kable* function is used to obtain a better visualization of the tables.
* **dplyr**: It has multiple functions that allow data management, some of the most interesting are those that allow you to manage tables as if it were a relational database, as is done with SQL.
* **ggplot2**: Perhaps the most important library for the creation of statistical graphics in R.
* **scales**: It is used in conjunction with ggplot2 to scale axes within the plot.

## Loading datasets

The original datasets are 5, named as in the code. In addition, there is a sixth one called "cruzada" (a cross of all 5 tables), which has binary information of aisles of a supermarket in the columns and rows representing the customers. The value 1 means that the customer has shopped in that aisle and 0 the opposite.

The calculation of the "cross" dimension of the dataset is included first.

```{r datasets}
order <- read.csv("Order.csv")
order_product <- read.csv("Order_Product.csv")
product <- read.csv("Product.csv")
aisle <- read.csv("Aisle.csv")
department <- read.csv("Department.csv")

cruzada <- as.data.frame(read.csv("cruzada_df.csv"))
```

```{r cruzada dim}
dimensions <- as.data.frame(matrix(dim(cruzada), ncol = 2))
names(dimensions) <- c("customers (rows)", "aisles (columns)")
kable(dimensions, row.names = FALSE, align = "cc")
```

The datasets are initially from a supermarket and cannot be shared due to source privacy issues.

## Extract the first 30 aisles with the most sales

Why only take the first 30 aisles into account?
The idea is to reduce the dimensionality of the "cruzada" dataset, so the first 30 aisles with the most sales are taken.

This is possible through the following 4 steps:

* **Join tables to map each product_id to its aisle_id:**

```{r}
order_product_info <- order_product %>%
  inner_join(product, by = "product_id")
```

* **Counting sales per aisle:**

```{r}
aisle_counts <- order_product_info %>%
  group_by(aisle_id) %>%
  summarise(sales_count = n()) %>%
  ungroup()
```

* **Sort and select the first 30 aisles:**

```{r}
top_aisles <- aisle_counts %>%
  arrange(desc(sales_count)) %>%
  slice(1:30)
```

* **Relate 'aisle_id' to aisle names:**

```{r}
top_aisles <- top_aisles %>%
  inner_join(aisle, by = "aisle_id")
```

And finally, we can create a plot that contains the distribution of this data in descending order.

```{r}
ggplot(top_aisles, aes(x = reorder(aisle, -sales_count), y = sales_count)) +
  geom_bar(stat = "identity", fill = "blue") +
  scale_y_continuous(labels = comma) + # This will change the format of the numbers on the y-axis.
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Aisle", y = "Number of sales", title = "Aisle sales")
```
It is possible to see that the aisles with the most purchases are *fresh fruits*, *fresh vegetables*, *packaged vegetables fruits*, and *yogurt*, all with more than 1 million sales.

## Reduction of variables in "cruzada" table

It is necessary to reduce the dimensionality of the "cruzada" table from 134 variables to only 30, the 30 we found in the immediate previous step. This can be done in 3 steps:

* **Replace spaces with periods in corridor names:**

```{r}
top_aisles$aisle <- gsub(" ", ".", top_aisles$aisle)
```

* **Extract aisle names from top_aisles:**

```{r}
top_aisle_names <- top_aisles$aisle
```

* **Filter 'cruzada' variables to include only those of top_aisle_names:**

```{r}
reduced_cruzada <- cruzada %>%
  select(all_of(top_aisle_names))
```

In this way, the new table has 30 variables and 206209 elements (rows).

## Generate a random sample

Generating a simple random sampling is vital to reduce the dimensionality corresponding to the number of rows since biplot methods present the difficulty that the more rows and columns there are, the more difficult it is to interpret the results since everything becomes difficult to read. graphically.

Consequently, five random samples were created from reduced_cross. The samples have 300, 400, 500, 700, and 1000 elements and were created using the following code:

```{r}
# 300 sample
set.seed(300)
random_dataset_300 <- reduced_cruzada[sample(nrow(reduced_cruzada), size=300),]

# 400 sample
set.seed(400)
random_dataset_400 <- reduced_cruzada[sample(nrow(reduced_cruzada), size=400),]

# 500 sample
set.seed(500)
random_dataset_500 <- reduced_cruzada[sample(nrow(reduced_cruzada), size=500),]

# 700 sample
set.seed(700)
random_dataset_700 <- reduced_cruzada[sample(nrow(reduced_cruzada), size=700),]

# 1000 sample
set.seed(1000)
random_dataset_1000 <- reduced_cruzada[sample(nrow(reduced_cruzada), size=1000),]
```

It is important to remember the use of the seed since it is what allows the replicability of the random sample, permanently storing the same dataframe that has been defined within the seed and avoiding the generation of random sampling that we will not be able to replicate later.

## Validation of samples

It is not advisable to use random samples directly without first performing some validation that these are representative samples of the dataset that we have chosen as the population.

Below is the code where it is seen that a graphic technique was applied and another chi-square test that yields a p-value to check whether the samples meet the conditions of representativeness statistically.

* **Function to calculate proportions and create a dataframe:**

```{r}
create_proportions_df <- function(complete_dataset, sample_dataset, suffix) {
  proportions_complete <- colSums(complete_dataset) / nrow(complete_dataset)
  proportions_sample <- colSums(sample_dataset) / nrow(sample_dataset)
  
  # Create a dataframe with the proportions of the sample and the complete set
  proportions_df <- data.frame(
    Aisle = rep(names(proportions_complete), 2),
    Proportion = c(proportions_complete, proportions_sample),
    Type = rep(c("Complete", "Sample"), each = length(proportions_complete))
  )
  # Convert 'Type' to a factor with consistent levels for the legend
  proportions_df$Type <- factor(proportions_df$Type, levels = c("Complete", "Sample"))
  
  return(proportions_df)
}
```

As can be seen, since this is binary data, we are working on proportions, and these are compared with each other.

* **Function to plot proportions:**

```{r}
plot_proportions <- function(proportions_df, suffix) {
  ggplot(proportions_df, aes(x = Aisle, y = Proportion, fill = Type)) +
    geom_bar(stat = "identity", position = position_dodge(width = 0.7)) +
    scale_fill_manual(
      values = c("Complete" = "blue", "Sample" = "red"),
      labels = c("Complete dataset", paste("Sample", suffix))
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    labs(y = "Proportion of Sales", x = "Aisles", fill = "Type",
         title = paste("Comparison of Proportions of Sales between Full and Sample Datasets", suffix))
}
```

* **Function to perform chi-square test and return adjusted and unadjusted p-values:**

The chi-square test is run with and without adjustment. This adjustment uses the Bonferroni technique to prevent type 1 errors when multiple comparisons are made simultaneously. Both types are presented because it can be helpful to see how the result changes.

```{r}
perform_chi_square_test <- function(complete_dataset, sample_dataset) {
  successes_complete <- colSums(complete_dataset)
  failures_complete <- nrow(complete_dataset) - successes_complete
  successes_sample <- colSums(sample_dataset)
  failures_sample <- nrow(sample_dataset) - successes_sample
  
  results_chi_square <- sapply(names(successes_complete), function(name) {
    contingency_table <- matrix(c(successes_complete[name], successes_sample[name],
                                   failures_complete[name], failures_sample[name]),
                                 nrow = 2)
    chisq.test(contingency_table)$p.value
  })
  
  adjusted_p_values <- p.adjust(results_chi_square, method = "bonferroni")
  
  # Return a list with adjusted and unadjusted p-values
  return(list(unadjusted_p_values = results_chi_square, adjusted_p_values = adjusted_p_values))
}
```

Some other complementary variables to continue with the execution of the code are the following:

```{r}
# List for storing charts
chart_list <- list()

# Dictionary for storing chi-square test results (adjusted and unadjusted)
results_p_values <- list()

# Process samples, plot and perform chi-square tests
suffixes <- c("300", "400", "500", "700", "1000")

for(suffix in suffixes) {
  sample_dataset <- get(paste("random_dataset_", suffix, sep = ""))
  proportions_df <- create_proportions_df(reduced_cruzada, sample_dataset, suffix)
  chart <- plot_proportions(proportions_df, suffix)
  chart_list[[suffix]] <- chart # Store the chart in the list
  
  results <- perform_chi_square_test(reduced_cruzada, sample_dataset)
  results_p_values[[suffix]] <- results
}
```

The sample validation graphs are printed below, where the proportions are compared:

```{r}
lapply(chart_list, print)
```

The next step is to print both the unadjusted and adjusted p-values for all samples.

```{r}
lapply(names(results_p_values), function(suffix) {
  results <- results_p_values[[suffix]]
  cat(paste("Results for sample size:", suffix, "\n"))
  
  # Unadjusted P-values
  cat("Unadjusted P-values:\n")
  print(kable(as.data.frame(results$unadjusted_p_values), format = "markdown"))
  
  # Adjusted P-values (Bonferroni)
  cat("\nAdjusted P-values (Bonferroni):\n")
  print(kable(as.data.frame(results$adjusted_p_values), format = "markdown"))
  
  cat("\n---\n")
})
```

As can be seen, all the adjusted p-values are one or give a very close value. Therefore, these random samples can be used, and the result will be representative.

## Logistic Biplot

The final step in the exploratory analysis of the customers' behavior is implementing a Binary Logistic Biplot. This technique allows the representation of the variables and the subjects in a reduced dimensional space.

Vicente-Villardón (2006) developed a Binary Logistic Biplot as a solution to implement a Biplot for binary data. It facilitates the interpretation of the results and offers an alternative for binary data exploration.

For this, the code to generate the Biplot of all the samples is presented below.

```{r}
for(suffix in suffixes) {
  # Construct the name of the current dataset
  dataset_name <- paste("random_dataset_", suffix, sep = "")
  
  # Accessing the dataset using get()
  current_dataset <- get(dataset_name)
  
  # Apply BinaryLogBiplotGD function to current dataset
  newBinaryData <- BinaryLogBiplotGD(current_dataset)
  
  # Generate the plot for the current dataset
  plot(newBinaryData, Mode="ah", ShowAxis = TRUE, margin = 0.4, AbbreviateLabels = FALSE,
       Significant = FALSE, LabelRows = FALSE)
}
```

In the sample size of 300, the points seem to be a little more dispersed, but as the sample increases, the points tend to become more concentrated, and even their orientation rotates a little in the direction of dimension 2. So, it is possible that dimension 2 better explains the distribution of the elements in the sample size of 1000, while in the sample size of 300, the same thing happens, but with dimension 1.

Considering that the shortest vectors represent the best-represented variables and that the Bonferroni adjustment has been applied to the categories, all have either a good or acceptable representation. The worst represented are the variables: bread and eggs in the sample of 300; soft.drinks in sample 400; frozen.meals in the sample of 500; toilet, seltzer, bread, and cream in the sample of 700; and water.seltzer, soup.broth, nuts.seeds and packaged.vegetables.fruits for the sample of 1000.

The result of the association between variables (aisles) can be seen in the next table, where they have been grouped by color and sample size so that associations have been created according to the graphic interpretation of each of the resulting biplots.






